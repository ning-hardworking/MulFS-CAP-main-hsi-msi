import os
import time
import math
import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.data as data
import scipy.io as sio
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

import args
from loss import loss as Loss
from model import model
from utils import utils

# å…¨å±€å¸¸é‡å®šä¹‰
model_name = "MulFS-CAP-HSI-MSI"
device_id = "0"
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def adjust_learning_rate(optimizer, epoch_count):
    lr = args.args.LR + 0.5 * (args.args.LR_target - args.args.LR) * (
            1 + math.cos((epoch_count - args.args.Warm_epoch) / (args.args.Epoch - args.args.Warm_epoch) * math.pi))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr


def warmup_learning_rate(optimizer, epoch_count):
    lr = epoch_count * ((args.args.LR_target - args.args.LR) / args.args.Warm_epoch) + args.args.LR
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr


class TrainDataset(data.Dataset):
    def __init__(self, hsi_dir, msi_dir, gt_dir,
                 hsi_deformed_dir, msi_deformed_dir, gt_deformed_dir,
                 transform=None, target_size=512, preload=True):
        super(TrainDataset, self).__init__()

        self.target_size = target_size
        self.transform = transform

        # Pair 1: åŸå§‹é…å‡†æ•°æ®
        self.hsi_paths = self.find_mat_files(hsi_dir)
        self.msi_paths = self.find_mat_files(msi_dir)
        self.gt_paths = self.find_mat_files(gt_dir)

        # Pair 2: å½¢å˜é…å‡†æ•°æ®
        self.hsi_d_paths = self.find_mat_files(hsi_deformed_dir)
        self.msi_d_paths = self.find_mat_files(msi_deformed_dir)
        self.gt_d_paths = self.find_mat_files(gt_deformed_dir)

        assert len(self.hsi_paths) == len(self.hsi_d_paths), \
            f"é…å¯¹æ•°æ®æ•°é‡ä¸ä¸€è‡´: Pair1={len(self.hsi_paths)}, Pair2={len(self.hsi_d_paths)}"

        # ========== âœ… æ ¸å¿ƒä¼˜åŒ–ï¼šé¢„åŠ è½½åˆ°å†…å­˜ ==========
        self.preload = preload
        self.data_cache = None

        if self.preload:
            print(f"\nğŸ”„ æ­£åœ¨é¢„åŠ è½½æ•°æ®é›†åˆ°å†…å­˜...")
            self.data_cache = []
            for idx in range(len(self.hsi_paths)):
                try:
                    # è¯»å–Pair 1
                    hsi_1 = self.read_mat_image(self.hsi_paths[idx])
                    msi_1 = self.read_mat_image(self.msi_paths[idx])
                    gt_1 = self.read_mat_image(self.gt_paths[idx])

                    # è¯»å–Pair 2
                    hsi_2 = self.read_mat_image(self.hsi_d_paths[idx])
                    msi_2 = self.read_mat_image(self.msi_d_paths[idx])
                    gt_2 = self.read_mat_image(self.gt_d_paths[idx])

                    # ========== ğŸ”¥ å…³é”®ï¼šé¢„å¤„ç†resize ==========
                    original_ratio = 32
                    hsi_target_size = self.target_size // original_ratio

                    # Resize MSIå’ŒGT
                    if msi_1.size(-1) != self.target_size:
                        msi_1 = F.interpolate(msi_1.unsqueeze(0), size=(self.target_size, self.target_size),
                                              mode='bilinear', align_corners=False).squeeze(0)
                        gt_1 = F.interpolate(gt_1.unsqueeze(0), size=(self.target_size, self.target_size),
                                             mode='bilinear', align_corners=False).squeeze(0)
                        msi_2 = F.interpolate(msi_2.unsqueeze(0), size=(self.target_size, self.target_size),
                                              mode='bilinear', align_corners=False).squeeze(0)
                        gt_2 = F.interpolate(gt_2.unsqueeze(0), size=(self.target_size, self.target_size),
                                             mode='bilinear', align_corners=False).squeeze(0)

                    # Resize HSI
                    if hsi_1.size(-1) != hsi_target_size:
                        hsi_1 = F.interpolate(hsi_1.unsqueeze(0), size=(hsi_target_size, hsi_target_size),
                                              mode='bilinear', align_corners=False).squeeze(0)
                        hsi_2 = F.interpolate(hsi_2.unsqueeze(0), size=(hsi_target_size, hsi_target_size),
                                              mode='bilinear', align_corners=False).squeeze(0)

                    self.data_cache.append((hsi_1, msi_1, gt_1, hsi_2, msi_2, gt_2))

                    if (idx + 1) % 5 == 0 or (idx + 1) == len(self.hsi_paths):
                        print(f"   å·²åŠ è½½ {idx + 1}/{len(self.hsi_paths)} å¯¹æ•°æ®")

                except Exception as e:
                    print(f"âŒ åŠ è½½ç¬¬{idx}å¯¹æ•°æ®å¤±è´¥: {str(e)}")
                    continue

            print(f"âœ… æ•°æ®é›†é¢„åŠ è½½å®Œæˆï¼å…±{len(self.data_cache)}å¯¹æ•°æ®\n")

        # æ‰“å°ä¿¡æ¯
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ (ç›®æ ‡å°ºå¯¸: {target_size}Ã—{target_size}):")
        print(f"   - Pair 1 (åŸå§‹é…å‡†): {len(self.hsi_paths)} å¯¹æ ·æœ¬")
        print(f"   - Pair 2 (å½¢å˜é…å‡†): {len(self.hsi_d_paths)} å¯¹æ ·æœ¬")
        print(f"   - é¢„åŠ è½½æ¨¡å¼: {'âœ… å·²å¯ç”¨' if self.preload else 'âŒ æœªå¯ç”¨'}")

        print(f"\nğŸ“ å°ºå¯¸å‚æ•°:")
        print(f"   - MSI/GTç›®æ ‡å°ºå¯¸: {target_size}Ã—{target_size}")
        print(f"   - HSIç›®æ ‡å°ºå¯¸: {target_size // 32}Ã—{target_size // 32}")
        print(f"   - ä¸‹é‡‡æ ·æ¯”ä¾‹: 32")

    def find_mat_files(self, dir_path):
        """æŸ¥æ‰¾æ‰€æœ‰.matæ–‡ä»¶"""
        mat_files = []
        for root, dirs, files in os.walk(dir_path):
            for file in files:
                if file.endswith('.mat'):
                    mat_files.append(os.path.join(root, file))
        mat_files.sort()
        return mat_files

    def read_mat_image(self, path, key=None):
        """è¯»å–.matæ–‡ä»¶ï¼ˆå·²ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            mat_data = sio.loadmat(path)

            if key is None:
                valid_keys = [k for k in mat_data.keys() if not k.startswith('__')]
                if len(valid_keys) == 0:
                    raise ValueError(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®é”®: {path}")
                key = valid_keys[0]

            img = mat_data[key]

            # æ™ºèƒ½è¯†åˆ«é€šé“ç»´åº¦
            if img.ndim == 2:
                img = torch.from_numpy(img).float().unsqueeze(0)
            elif img.ndim == 3:
                shape = img.shape
                expected_channels = [3, 31]
                channel_dim_idx = None

                for i, s in enumerate(shape):
                    if s in expected_channels:
                        channel_dim_idx = i
                        break

                if channel_dim_idx is not None:
                    target_dim = channel_dim_idx
                else:
                    target_dim = np.argmin(shape)

                img = np.moveaxis(img, source=target_dim, destination=0)
                img = torch.from_numpy(img).float()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç»´åº¦: {img.ndim}Dï¼Œè·¯å¾„: {path}")

            # å½’ä¸€åŒ–
            if img.max() > 1.0:
                img = img / img.max()

            if self.transform is not None:
                img = self.transform(img)

            return img

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {path}")
            print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            raise

    def __getitem__(self, index):
        # ========== âœ… ç›´æ¥ä»å†…å­˜è¯»å– ==========
        if self.preload and self.data_cache is not None:
            return self.data_cache[index]

        # ========== åŸå§‹é€»è¾‘ï¼ˆä¸é¢„åŠ è½½æ—¶ä½¿ç”¨ï¼‰==========
        # Pair 1: åŸå§‹é…å‡†å¯¹
        hsi_1 = self.read_mat_image(self.hsi_paths[index])
        msi_1 = self.read_mat_image(self.msi_paths[index])
        gt_1 = self.read_mat_image(self.gt_paths[index])

        # Pair 2: å½¢å˜é…å‡†å¯¹
        hsi_2 = self.read_mat_image(self.hsi_d_paths[index])
        msi_2 = self.read_mat_image(self.msi_d_paths[index])
        gt_2 = self.read_mat_image(self.gt_d_paths[index])

        # ========== ğŸ”¥ Resizeåˆ°ç›®æ ‡å°ºå¯¸ ğŸ”¥ ==========
        original_ratio = 32
        hsi_target_size = self.target_size // original_ratio

        # Resize MSIå’ŒGT
        if msi_1.size(-1) != self.target_size:
            msi_1 = F.interpolate(msi_1.unsqueeze(0), size=(self.target_size, self.target_size),
                                  mode='bilinear', align_corners=False).squeeze(0)
            gt_1 = F.interpolate(gt_1.unsqueeze(0), size=(self.target_size, self.target_size),
                                 mode='bilinear', align_corners=False).squeeze(0)
            msi_2 = F.interpolate(msi_2.unsqueeze(0), size=(self.target_size, self.target_size),
                                  mode='bilinear', align_corners=False).squeeze(0)
            gt_2 = F.interpolate(gt_2.unsqueeze(0), size=(self.target_size, self.target_size),
                                 mode='bilinear', align_corners=False).squeeze(0)

        # Resize HSI
        if hsi_1.size(-1) != hsi_target_size:
            hsi_1 = F.interpolate(hsi_1.unsqueeze(0), size=(hsi_target_size, hsi_target_size),
                                  mode='bilinear', align_corners=False).squeeze(0)
            hsi_2 = F.interpolate(hsi_2.unsqueeze(0), size=(hsi_target_size, hsi_target_size),
                                  mode='bilinear', align_corners=False).squeeze(0)

        return hsi_1, msi_1, gt_1, hsi_2, msi_2, gt_2

    def __len__(self):
        return len(self.hsi_paths)


# æ ¸å¿ƒï¼šæ‰€æœ‰æ‰§è¡Œé€»è¾‘å¿…é¡»åŒ…è£¹åˆ°if __name__ == '__main__'ä¸­
if __name__ == '__main__':
    # ========== åˆå§‹åŒ–ç¯å¢ƒ ==========
    os.environ['CUDA_LAUNCH_BLOCKING'] = device_id
    device = torch.device("cuda:" + device_id if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    try:
        torch.multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass

    # ========== åˆå§‹åŒ–ä¿å­˜ç›®å½• ==========
    now = int(time.time())
    timeArr = time.localtime(now)
    nowTime = time.strftime("%Y%m%d_%H-%M-%S", timeArr)
    save_model_dir = args.args.train_save_model_dir + "/" + nowTime + "_" + model_name + "_model"
    save_img_dir = args.args.train_save_img_dir + "/" + nowTime + "_" + model_name + "_img"
    utils.check_dir(save_model_dir)
    utils.check_dir(save_img_dir)

    # ========== æ•°æ®åŠ è½½å™¨åˆå§‹åŒ– ==========

    # âœ… ä¿®æ”¹æ•°æ®é›†åˆå§‹åŒ–
    tf = None

    dataset = TrainDataset(
        args.args.hsi_train_dir,
        args.args.msi_train_dir,
        args.args.gt_train_dir,
        args.args.hsi_deformed_train_dir,
        args.args.msi_deformed_train_dir,
        args.args.gt_deformed_train_dir,
        tf,
        target_size=args.args.img_size  # âœ… ä¼ å…¥ç›®æ ‡å°ºå¯¸
    )

    data_iter = data.DataLoader(
        dataset=dataset,
        shuffle=True,
        batch_size=args.args.batch_size,
        num_workers=4,
        pin_memory=True,
        drop_last=True,
        multiprocessing_context=torch.multiprocessing.get_context('spawn')
    )

    iter_num = int(dataset.__len__() / args.args.batch_size)
    save_image_iter = max(1, int(iter_num / args.args.save_image_num))

    # ========== æŸå¤±å‡½æ•°åˆå§‹åŒ– ==========
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æŸå¤±å‡½æ•°...")
    Lgrad = Loss.L_Grad().to(device)
    CC = Loss.CorrelationCoefficient().to(device)
    Lcorrespondence_static = Loss.L_correspondence_static().to(device)  # å®Œæ•´ç‰ˆ

    # âœ… æ–°å¢ï¼š31é€šé“ä¸“ç”¨æŸå¤±å‡½æ•°
    SpectralLoss = Loss.SpectralConsistencyLoss().to(device)
    SAMLoss = Loss.SpectralAngleLoss().to(device)
    print("âœ… å·²åŠ è½½31é€šé“ä¸“ç”¨æŸå¤±å‡½æ•°")

    # âœ… å…³é”®ä¿®å¤1: åˆ›å»ºä¸¤ä¸ªä¸åŒçš„baseæ¨¡å—
    with torch.no_grad():
        base_msi = model.base(in_channels=3)  # MSI: 3é€šé“ -> 64é€šé“
        base_hsi = model.base(in_channels=31)  # HSI: 31é€šé“ -> 64é€šé“
        hsi_MFE = model.FeatureExtractor()
        msi_MFE = model.FeatureExtractor()
        fusion_decoder = model.Decoder()
        PAFE = model.FeatureExtractor()
        decoder = model.Decoder()
        MN_hsi = model.Enhance()
        MN_msi = model.Enhance()
        HSIDP = model.DictionaryRepresentationModule()
        MSIDP = model.DictionaryRepresentationModule()
        MHCSA_hsi = model.MHCSAB()
        MHCSA_msi = model.MHCSAB()
        fusion_module = model.FusionMoudle()

    # æ¨¡å‹è®­ç»ƒæ¨¡å¼+è®¾å¤‡è¿ç§»
    print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°GPU...")
    base_msi.train().to(device)
    base_hsi.train().to(device)
    hsi_MFE.train().to(device)
    msi_MFE.train().to(device)
    fusion_decoder.train().to(device)
    PAFE.train().to(device)
    decoder.train().to(device)
    HSIDP.train().to(device)
    MSIDP.train().to(device)
    MN_hsi.train().to(device)
    MN_msi.train().to(device)
    MHCSA_hsi.train().to(device)
    MHCSA_msi.train().to(device)
    fusion_module.train().to(device)

    # ========== ä¼˜åŒ–å™¨åˆå§‹åŒ– ==========
    print("âš™ï¸ æ­£åœ¨é…ç½®ä¼˜åŒ–å™¨...")
    optimizer_FE = torch.optim.Adam([
        {'params': base_msi.parameters()},  # âœ… ä¿®å¤2: åŒ…å«ä¸¤ä¸ªbase
        {'params': base_hsi.parameters()},
        {'params': hsi_MFE.parameters()},
        {'params': msi_MFE.parameters()},
        {'params': fusion_decoder.parameters()},
        {'params': PAFE.parameters()},
        {'params': decoder.parameters()},
        {'params': MN_hsi.parameters()},
        {'params': MN_msi.parameters()}
    ], lr=0.0002)

    optimizer_HSIDP = torch.optim.Adam(HSIDP.parameters(), lr=0.0008)
    optimizer_MSIDP = torch.optim.Adam(MSIDP.parameters(), lr=0.0008)
    optimizer_MHCSAhsi = torch.optim.Adam(MHCSA_hsi.parameters(), lr=args.args.LR)
    optimizer_MHCSAmsi = torch.optim.Adam(MHCSA_msi.parameters(), lr=args.args.LR)
    optimizer_FusionModule = torch.optim.Adam(fusion_module.parameters(), lr=0.0002)

    # âœ… ä¼˜åŒ–3: æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler()
    print("âœ… å·²å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰ï¼Œæ˜¾å­˜å ç”¨å°†å‡å°‘çº¦50%")


    # ========== è®­ç»ƒå‡½æ•°å®šä¹‰ ==========
    def train(epoch):
        # ========== âœ… å®Œæ•´çš„ç»´åº¦å’Œå‚æ•°éªŒè¯ ==========
        if epoch == 0:
            print(f"\n{'=' * 70}")
            print(f"ğŸ” Epoch 0 - å®Œæ•´å‚æ•°éªŒè¯:")
            print(f"{'=' * 70}")

        epoch_loss_HSIDP = []
        epoch_loss_MSIDP = []
        epoch_loss_same = []
        epoch_loss_fusion_total = []

        for step, x in enumerate(data_iter):
            # ========== æ•°æ®åŠ è½½ï¼ˆ6ä¸ªå¼ é‡ï¼‰==========
            hsi_1, msi_1, gt_1, hsi_2, msi_2, gt_2 = [
                item.to(device, non_blocking=True) for item in x
            ]

            # âœ… æ‰“å°ç»´åº¦ï¼ˆä»…ç¬¬ä¸€ä¸ªbatchï¼‰
            if step == 0 and epoch == 0:
                # âœ… åŠ¨æ€è®¡ç®—æœŸæœ›å°ºå¯¸
                expected_msi_size = args.args.img_size
                expected_hsi_size = args.args.img_size // 32

                print(f"\nâœ… æ•°æ®ç»´åº¦éªŒè¯:")
                print(f"   Pair 1:")
                print(f"     - HSI:  {hsi_1.shape}  (æœŸæœ›: [B, 31, {expected_hsi_size}, {expected_hsi_size}])")
                print(f"     - MSI:  {msi_1.shape}  (æœŸæœ›: [B, 3, {expected_msi_size}, {expected_msi_size}])")
                print(f"     - GT:   {gt_1.shape}   (æœŸæœ›: [B, 31, {expected_msi_size}, {expected_msi_size}])")
                print(f"   Pair 2:")
                print(f"     - HSI:  {hsi_2.shape}  (æœŸæœ›: [B, 31, {expected_hsi_size}, {expected_hsi_size}])")
                print(f"     - MSI:  {msi_2.shape}  (æœŸæœ›: [B, 3, {expected_msi_size}, {expected_msi_size}])")
                print(f"     - GT:   {gt_2.shape}   (æœŸæœ›: [B, 31, {expected_msi_size}, {expected_msi_size}])")

                # âœ… éªŒè¯GTç¡®å®æ˜¯31é€šé“
                assert gt_1.size(1) == 31, f"âŒ GTé€šé“æ•°é”™è¯¯ï¼æœŸæœ›31,å®é™…{gt_1.size(1)}"
                assert gt_2.size(1) == 31, f"âŒ GTé€šé“æ•°é”™è¯¯ï¼æœŸæœ›31,å®é™…{gt_2.size(1)}"
                print(f"âœ… æ‰€æœ‰ç»´åº¦åŒ¹é…ï¼\n")
            # ========== ä¸Šé‡‡æ ·HSIåˆ°GTçš„åˆ†è¾¨ç‡ ==========
            hsi_1_up = F.interpolate(
                hsi_1,
                size=(msi_1.size(2), msi_1.size(3)),
                mode='bilinear',
                align_corners=False
            )
            hsi_2_up = F.interpolate(
                hsi_2,
                size=(msi_2.size(2), msi_2.size(3)),
                mode='bilinear',
                align_corners=False
            )

            # ========== æ··åˆç²¾åº¦è®­ç»ƒ ==========
            with autocast():
                # ====================================================================
                # é˜¶æ®µ1: åŸºç¡€ç‰¹å¾æå–ï¼ˆ64é€šé“ç»Ÿä¸€ç‰¹å¾ç©ºé—´ï¼‰
                # ====================================================================
                hsi_1_base = base_hsi(hsi_1_up)  # hsi_1_up(1, 31, 512, 512) -> hsi_1_base(1, 64, 512, 512)
                msi_1_base = base_msi(msi_1)  # msi_1(1, 3, 512, 512)  -> msi_1_base(1, 64, 512, 512)
                hsi_2_base = base_hsi(hsi_2_up)  # hsi_2_up(1, 31, 512, 512) ->hsi_2_base (1, 64,512, 512)
                msi_2_base = base_msi(msi_2)  # msi_2(1, 3, 512, 512)  -> msi_2_base(1, 64, 512, 512)



                # ====================================================================
                # é˜¶æ®µ2: æ·±å±‚ç‰¹å¾æå–ï¼ˆç”¨äºèåˆé‡å»ºï¼‰
                # ====================================================================
                # Pair 1 çš„æ·±å±‚ç‰¹å¾
                hsi_1_fe = hsi_MFE(hsi_1_base)  # (B, 64, 512, 512) -> hsi_1_fe(B, 64, 512, 512)  B=1
                msi_1_fe = msi_MFE(msi_1_base)  # (B, 64, 128, 128) -> msi_1_fe (B, 64, 512, 512)  B=1
                simple_fusion_f_1 = hsi_1_fe + msi_1_fe      # ç®€å•ç›¸åŠ èåˆ
                fusion_image_1, fusion_f_1 = fusion_decoder(simple_fusion_f_1)  # æœ€ç»ˆé¢„æµ‹çš„é«˜å…‰è°±å›¾åƒfusion_image_1-> (B, 31, 512, 512)  è§£ç å™¨ä¸­é—´ç‰¹å¾fusion_f_1-> (B, 64, 512, 512)  B=1

                # Pair 2 çš„æ·±å±‚ç‰¹å¾
                hsi_2_fe = hsi_MFE(hsi_2_base)  # (B, 64, 512, 512) -> hsi_1_fe(B, 64, 512, 512)  B=1
                msi_2_fe = msi_MFE(msi_2_base)  # (B, 64, 128, 128) -> msi_1_fe (B, 64, 512, 512)  B=1
                simple_fusion_f_2 = hsi_2_fe + msi_2_fe
                fusion_image_2, fusion_f_2 = fusion_decoder(simple_fusion_f_2)  # æœ€ç»ˆé¢„æµ‹çš„é«˜å…‰è°±å›¾åƒfusion_image_2-> (B, 31, 512, 512)  è§£ç å™¨ä¸­é—´ç‰¹å¾fusion_f_2-> (B, 64, 512, 512)  B=1


                # ====================================================================
                # é˜¶æ®µ3: PAFEç‰¹å¾æå–ï¼ˆç”¨äºå¯¹é½æ„ŸçŸ¥ï¼‰
                # ====================================================================
                # Pair 1 çš„PAFEç‰¹å¾
                hsi_1_f = PAFE(hsi_1_base)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                msi_1_f = PAFE(msi_1_base)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                simple_fusion_pf_1 = hsi_1_f + msi_1_f
                fusion_pimage_1, fusion_pf_1 = decoder(simple_fusion_pf_1)  # -> (B, 31, 512, 512)

                # Pair 2 çš„PAFEç‰¹å¾
                hsi_2_f = PAFE(hsi_2_base)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                msi_2_f = PAFE(msi_2_base)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                simple_fusion_pf_2 = hsi_2_f + msi_2_f
                fusion_pimage_2, fusion_pf_2 = decoder(simple_fusion_pf_2)  # -> (B, 31, 512, 512)


                # ====================================================================
                # é˜¶æ®µ4: æ¨¡æ€å½’ä¸€åŒ–ï¼ˆModality Normalizationï¼‰
                # ====================================================================
                hsi_1_e_f = MN_hsi(hsi_1_f)  # (B, 64, 512, 128) -> (B, 64, 512, 512)
                msi_1_e_f = MN_msi(msi_1_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                hsi_2_e_f = MN_hsi(hsi_2_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                msi_2_e_f = MN_msi(msi_2_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)

                # ====================================================================
                # é˜¶æ®µ5: å­—å…¸è¡¨ç¤ºæ¨¡å—ï¼ˆDictionary Representation Moduleï¼‰
                # ç”¨å¯å­¦ä¹ çš„æ¨¡æ€å­—å…¸è¡¥å¿å•æ¨¡æ€ç‰¹å¾ç¼ºå¤±çš„ä¿¡æ¯
                # ====================================================================
                HSIDP_hsi_1_f, _ = HSIDP(hsi_1_e_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                MSIDP_msi_1_f, _ = MSIDP(msi_1_e_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                HSIDP_hsi_2_f, _ = HSIDP(hsi_2_e_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)
                MSIDP_msi_2_f, _ = MSIDP(msi_2_e_f)  # (B, 64, 512, 512) -> (B, 64, 512, 512)


                # ====================================================================
                # é˜¶æ®µ6: è·¨æ¨¡æ€å¯¹é½æ„ŸçŸ¥ï¼ˆCross-Modality Alignment Perceptionï¼‰
                # æ ¸å¿ƒï¼šæ„é€ æœªé…å‡†å¯¹ï¼ˆPair1çš„HSI + Pair2çš„MSIï¼‰
                # ====================================================================
                # ğŸ”¥ å…³é”®è®¾è®¡ï¼šç”¨Pair1çš„HSIä½œä¸ºå‚è€ƒï¼ˆfixedï¼‰ï¼ŒPair2çš„MSIä½œä¸ºç§»åŠ¨ï¼ˆmovingï¼‰
                # è¿™æ ·å¯ä»¥å­¦ä¹ å¦‚ä½•å°†æœªé…å‡†çš„MSIå¯¹é½åˆ°HSI
                fixed_DP = HSIDP_hsi_1_f  # å‚è€ƒå›¾åƒç‰¹å¾ï¼ˆæ¥è‡ªPair1ï¼‰
                moving_DP = MSIDP_msi_2_f  # ç§»åŠ¨å›¾åƒç‰¹å¾ï¼ˆæ¥è‡ªPair2ï¼‰

                # çª—å£åˆ†å‰²
                moving_DP_lw = model.df_window_partition(
                    moving_DP,
                    args.args.large_w_size,  # 12
                    args.args.small_w_size  # 8
                )  # -> (num_windows, B, 64, 12, 12)

                fixed_DP_sw = model.window_partition(
                    fixed_DP,
                    args.args.small_w_size,  # 8
                    args.args.small_w_size  # 8
                )  # -> (num_windows, B, 64, 8, 8)

                # è®¡ç®—å¯¹é½æ„ŸçŸ¥çŸ©é˜µ
                correspondence_matrixs = model.CMAP(
                    fixed_DP_sw,  # å‚è€ƒçª—å£
                    moving_DP_lw,  # ç§»åŠ¨çª—å£
                    MHCSA_hsi,  # HSIçš„å¤šå¤´è·¨å°ºåº¦æ³¨æ„åŠ›
                    MHCSA_msi,  # MSIçš„å¤šå¤´è·¨å°ºåº¦æ³¨æ„åŠ›
                    True  # HSIä½œä¸ºå‚è€ƒ
                )  # -> (num_windows, B, 8*8, 12*12)

                # ====================================================================
                # é˜¶æ®µ7: ç‰¹å¾é‡ç»„å’Œæœ€ç»ˆèåˆ
                # æ ¹æ®å¯¹é½çŸ©é˜µé‡ç»„MSIç‰¹å¾ï¼Œä½¿å…¶ä¸HSIå¯¹é½
                # ====================================================================
                msi_2_f_sample = model.feature_reorganization(
                    correspondence_matrixs,  # å¯¹é½çŸ©é˜µ
                    msi_2_fe  # Pair2çš„MSIç‰¹å¾
                )  # -> (B, 64, 128, 128) - å¯¹é½åçš„MSIç‰¹å¾

                # æœ€ç»ˆèåˆï¼šPair1çš„HSI + å¯¹é½åçš„Pair2çš„MSI
                fusion_image_sample = fusion_module(
                    hsi_1_fe,  # Pair1çš„HSIç‰¹å¾
                    msi_2_f_sample  # å¯¹é½åçš„Pair2çš„MSIç‰¹å¾
                )  # -> (B, 31, 128, 128)

                # ====================================================================
                # é˜¶æ®µ8: æŸå¤±è®¡ç®—
                # ====================================================================

                # 8.1 åŸºç¡€èåˆæŸå¤±ï¼ˆç›‘ç£ä¸¤å¯¹é…å‡†æ•°æ®çš„èåˆè´¨é‡ï¼‰
                # âœ… ä¿®æ­£ï¼šä¼ å…¥æ­£ç¡®çš„ (hsi, msi, fusion) ä¸‰å…ƒç»„
                # 8.1 åŸºç¡€èåˆæŸå¤±ï¼ˆç›‘ç£ä¸¤å¯¹é…å‡†æ•°æ®çš„èåˆè´¨é‡ï¼‰
                loss_fusion_1 = (
                        Lgrad(hsi_1, msi_1, fusion_image_1) +
                        Loss.Loss_intensity(hsi_1, msi_1, fusion_image_1) +
                        Lgrad(hsi_1, msi_1, fusion_pimage_1) +
                        Loss.Loss_intensity(hsi_1, msi_1, fusion_pimage_1) +
                        0.5 * SpectralLoss(fusion_image_1, gt_1)
                )

                loss_fusion_2 = (
                        Lgrad(hsi_2, msi_2, fusion_image_2) +
                        Loss.Loss_intensity(hsi_2, msi_2, fusion_image_2) +
                        Lgrad(hsi_2, msi_2, fusion_pimage_2) +
                        Loss.Loss_intensity(hsi_2, msi_2, fusion_pimage_2) +
                        0.5 * SpectralLoss(fusion_image_2, gt_2)
                )

                loss_0 = loss_fusion_1 + loss_fusion_2

                # 8.2 å­—å…¸ä¸€è‡´æ€§æŸå¤±
                loss_HSIDP = (
                        - CC(HSIDP_hsi_1_f, fusion_pf_1.detach())
                        - CC(HSIDP_hsi_2_f, fusion_pf_2.detach())
                )

                loss_MSIDP = (
                        - CC(MSIDP_msi_1_f, fusion_pf_1.detach())
                        - CC(MSIDP_msi_2_f, fusion_pf_2.detach())
                )

                # 8.3 æ¨¡æ€ä¸€è‡´æ€§æŸå¤±
                loss_same = (
                        F.mse_loss(HSIDP_hsi_1_f, MSIDP_msi_1_f) +
                        F.mse_loss(HSIDP_hsi_2_f, MSIDP_msi_2_f)
                )

                loss_1 = 2 * (loss_HSIDP + loss_MSIDP + 0.5 * loss_same)

                # 8.4 å¯¹é½èåˆæŸå¤±
                loss_2 = (
                        Lgrad(hsi_1, msi_2, fusion_image_sample) +
                        Loss.Loss_intensity(hsi_1, msi_2, fusion_image_sample)
                )

                # 8.5 å¯¹é½ç›‘ç£æŸå¤±ï¼ˆâœ… æ–°å¢ï¼šé™æ€ç‰ˆæœ¬ï¼‰
                loss_correspondence, corr_loss_dict = Lcorrespondence_static(
                    correspondence_matrixs,  # (num_windows, B, sw^2, lw^2)
                    fusion_image_sample,  # (B, 31, H, W)
                    hsi_1,  # (B, 31, H_hsi, W_hsi)
                    msi_2,  # (B, 3, H, W)
                    gt_1  # (B, 31, H, W)
                )

                loss_3 = 2.0 * loss_correspondence

                # æ€»æŸå¤±
                loss = loss_0 + loss_1 + loss_2 + loss_3

            # ========== åå‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰==========
            optimizer_HSIDP.zero_grad()
            optimizer_MSIDP.zero_grad()
            optimizer_MHCSAhsi.zero_grad()
            optimizer_MHCSAmsi.zero_grad()
            optimizer_FusionModule.zero_grad()
            optimizer_FE.zero_grad()

            scaler.scale(loss).backward()
            scaler.step(optimizer_FE)
            scaler.step(optimizer_HSIDP)
            scaler.step(optimizer_MSIDP)
            scaler.step(optimizer_MHCSAhsi)
            scaler.step(optimizer_MHCSAmsi)
            scaler.step(optimizer_FusionModule)
            scaler.update()



            # ========== è®°å½•æŸå¤± ==========
            epoch_loss_HSIDP.append(loss_HSIDP.item())
            epoch_loss_MSIDP.append(loss_MSIDP.item())
            epoch_loss_same.append(loss_same.item())
            epoch_loss_fusion_total.append(loss.item())

            # ========== ä¿å­˜æ¨¡å‹ ==========
            if ((epoch + 1) == args.args.Epoch and (step + 1) % iter_num == 0) or \
                    (epoch % args.args.save_model_num == 0 and (step + 1) % iter_num == 0):
                ckpts = {
                    "bfe_msi": base_msi.state_dict(),
                    "bfe_hsi": base_hsi.state_dict(),
                    "msi_mfe": msi_MFE.state_dict(),
                    "hsi_mfe": hsi_MFE.state_dict(),
                    "pafe": PAFE.state_dict(),
                    "fusion_decoder": fusion_decoder.state_dict(),
                    "decoder": decoder.state_dict(),
                    "mn_msi": MN_msi.state_dict(),
                    "mn_hsi": MN_hsi.state_dict(),
                    "msi_dgfp": MSIDP.state_dict(),
                    "hsi_dgfp": HSIDP.state_dict(),
                    "mhcsab_msi": MHCSA_msi.state_dict(),
                    "mhcsab_hsi": MHCSA_hsi.state_dict(),
                    "fusion_block": fusion_module.state_dict(),
                }
                save_dir = '{:s}/epoch{:d}_iter{:d}.pth'.format(save_model_dir, epoch, step + 1)
                torch.save(ckpts, save_dir)
                print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_dir}")



            # âœ… æ¯10æ­¥æ‰“å°æ˜¾å­˜ï¼ˆå¯é€‰ï¼‰
            if step % max(iter_num // 3, 1) == 0 or step == 0:  # æ¯ä¸ªepochåªæ‰“å°3æ¬¡
                print(f"Step {step}/{iter_num} - æ˜¾å­˜: ...")

        # ========== æ‰“å°epochç»Ÿè®¡ä¿¡æ¯ ==========
        epoch_loss_HSIDP_mean = np.mean(epoch_loss_HSIDP)
        epoch_loss_MSIDP_mean = np.mean(epoch_loss_MSIDP)
        epoch_loss_same_mean = np.mean(epoch_loss_same)
        epoch_loss_fusion_mean = np.mean(epoch_loss_fusion_total)

        # âœ… æ–°å¢ï¼šè®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªbatchçš„ç»“æœï¼‰
        with torch.no_grad():
            # è®¡ç®—PSNR
            mse = F.mse_loss(fusion_image_sample, gt_1)
            psnr = 10 * torch.log10(1.0 / mse)

            # è®¡ç®—SAMï¼ˆå…‰è°±è§’è·ç¦»ï¼‰
            # å°†ç©ºé—´ç»´åº¦å±•å¹³: (B, 31, H, W) -> (B, 31, H*W)
            pred_flat = fusion_image_sample.view(fusion_image_sample.size(0), fusion_image_sample.size(1), -1)
            target_flat = gt_1.view(gt_1.size(0), gt_1.size(1), -1)

            # è®¡ç®—å†…ç§¯å’Œæ¨¡é•¿
            dot_product = torch.sum(pred_flat * target_flat, dim=1)  # (B, H*W)
            pred_norm = torch.norm(pred_flat, dim=1) + 1e-8
            target_norm = torch.norm(target_flat, dim=1) + 1e-8

            # è®¡ç®—coså€¼å¹¶è½¬æ¢ä¸ºè§’åº¦
            cos_theta = dot_product / (pred_norm * target_norm)
            cos_theta = torch.clamp(cos_theta, -1.0, 1.0)
            sam = torch.acos(cos_theta).mean() * 180 / np.pi  # è½¬æ¢ä¸ºåº¦æ•°

        # âœ… æ–°å¢ï¼šæ‰“å°å¯¹é½æŸå¤±è¯¦æƒ…
        print()
        print(f"ğŸ“Š Epoch {epoch} ç»Ÿè®¡:")
        print(f"   - æ€»æŸå¤±(Total Loss):     {epoch_loss_fusion_mean:.6f}")
        print(f"   - HSIå­—å…¸æŸå¤±(HSIDP):      {epoch_loss_HSIDP_mean:.6f}")
        print(f"   - MSIå­—å…¸æŸå¤±(MSIDP):      {epoch_loss_MSIDP_mean:.6f}")
        print(f"   - æ¨¡æ€ä¸€è‡´æ€§æŸå¤±(Same):    {epoch_loss_same_mean:.6f}")

        # âœ… æ‰“å°å¯¹é½æŸå¤±çš„è¯¦ç»†ä¿¡æ¯
        if 'corr_loss_dict' in locals():
            print(f"   ğŸ“ å¯¹é½æŸå¤±è¯¦æƒ…:")
            print(f"      - èåˆè´¨é‡: {corr_loss_dict['fusion']:.6f}")
            print(f"      - ç¨€ç–æ€§:   {corr_loss_dict['entropy']:.6f}")
            print(f"      - å…‰è°±:     {corr_loss_dict['spectral']:.6f}")
            print(f"      - æ€»è®¡:     {corr_loss_dict['total']:.6f}")

        print(f"   ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
        print(f"      - PSNR: {psnr.item():.4f} dB")
        print(f"      - SAM:  {sam.item():.4f}Â°")



    """
        è¾“å…¥: 6ä¸ªå¼ é‡
â”œâ”€â”€ Pair 1: hsi_1, msi_1, gt_1  (æ¥è‡ª Z_reconst, Y_reconst, X)
â””â”€â”€ Pair 2: hsi_2, msi_2, gt_2  (æ¥è‡ª Z_deformed, Y_deformed, X_deformed)

æ ¸å¿ƒè®¾è®¡:
â”œâ”€â”€ ç”¨Pair1å’ŒPair2åˆ†åˆ«è®­ç»ƒåŸºç¡€èåˆèƒ½åŠ›
â””â”€â”€ æ„é€ æœªé…å‡†å¯¹: hsi_1 + msi_2 (è·¨Pairæ„é€ )
    â””â”€â”€ é€šè¿‡CMAPå­¦ä¹ å¯¹é½å…³ç³»
        â””â”€â”€ ç”Ÿæˆæœ€ç»ˆå¯¹é½èåˆç»“æœ
        """



    # ========== å¯åŠ¨è®­ç»ƒå¾ªç¯ ==========
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“Œ è®­ç»ƒå‚æ•°:")
    print(f"   - Epochs: {args.args.Epoch}")
    print(f"   - Batch Size: {args.args.batch_size}")
    print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {len(dataset)}")
    print(f"   - æ¯epochè¿­ä»£æ•°: {iter_num}")
    print(f"   - å›¾åƒå°ºå¯¸: {args.args.img_size}Ã—{args.args.img_size}")
    print(f"   - æ··åˆç²¾åº¦: å·²å¯ç”¨\n")

    for epoch in tqdm(range(args.args.Epoch), desc="è®­ç»ƒè¿›åº¦"):
        if epoch < args.args.Warm_epoch:
            warmup_learning_rate(optimizer_MHCSAhsi, epoch)
            warmup_learning_rate(optimizer_MHCSAmsi, epoch)
        else:
            adjust_learning_rate(optimizer_MHCSAhsi, epoch)
            adjust_learning_rate(optimizer_MHCSAmsi, epoch)

        train(epoch)

    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")