"""
HSI-MSIèåˆæµ‹è¯•è„šæœ¬ï¼ˆå›ºå®šå°ºå¯¸ç‰ˆï¼‰
- æ‰€æœ‰å›¾åƒResizeåˆ°ç»Ÿä¸€å°ºå¯¸
- è®¡ç®—PSNRã€SAMç­‰è¯„ä¼°æŒ‡æ ‡
- ä¿å­˜èåˆç»“æœä¸º.matæ–‡ä»¶
"""

import os
from pathlib import Path
import gc

import torch
import torch.utils.data as data
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np
import scipy.io as sio

import model.model as model
import utils.utils as utils
import args

# ========== è®¾å¤‡é…ç½® ==========
device_id = "0"
os.environ['CUDA_LAUNCH_BLOCKING'] = device_id
device = torch.device("cuda:" + device_id if torch.cuda.is_available() else "cpu")

print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")


# ========== è¯„ä¼°æŒ‡æ ‡å‡½æ•°ï¼ˆå†…åµŒç‰ˆï¼‰==========
def calculate_psnr(pred, target, data_range=1.0):
    """è®¡ç®—PSNR"""
    mse = F.mse_loss(pred, target)
    if mse == 0:
        return float('inf')
    psnr = 10 * torch.log10(data_range ** 2 / mse)
    return psnr.item()


def calculate_sam(pred, target):
    """è®¡ç®—SAMï¼ˆå…‰è°±è§’è·ç¦»ï¼‰"""
    # å°†ç©ºé—´ç»´åº¦å±•å¹³: (B, C, H, W) -> (B, C, H*W)
    pred_flat = pred.view(pred.size(0), pred.size(1), -1)
    target_flat = target.view(target.size(0), target.size(1), -1)

    # è®¡ç®—å†…ç§¯
    dot_product = torch.sum(pred_flat * target_flat, dim=1)  # (B, H*W)

    # è®¡ç®—æ¨¡é•¿
    pred_norm = torch.norm(pred_flat, dim=1) + 1e-8
    target_norm = torch.norm(target_flat, dim=1) + 1e-8

    # è®¡ç®—coså€¼å¹¶è½¬æ¢ä¸ºè§’åº¦
    cos_theta = dot_product / (pred_norm * target_norm)
    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)
    sam = torch.acos(cos_theta).mean() * 180 / np.pi

    return sam.item()


def calculate_ssim(pred, target, data_range=1.0):
    """è®¡ç®—SSIM"""
    C1 = (0.01 * data_range) ** 2
    C2 = (0.03 * data_range) ** 2

    mu_pred = F.avg_pool2d(pred, 3, 1, 1)
    mu_target = F.avg_pool2d(target, 3, 1, 1)

    mu_pred_sq = mu_pred ** 2
    mu_target_sq = mu_target ** 2
    mu_pred_target = mu_pred * mu_target

    sigma_pred_sq = F.avg_pool2d(pred ** 2, 3, 1, 1) - mu_pred_sq
    sigma_target_sq = F.avg_pool2d(target ** 2, 3, 1, 1) - mu_target_sq
    sigma_pred_target = F.avg_pool2d(pred * target, 3, 1, 1) - mu_pred_target

    ssim_map = ((2 * mu_pred_target + C1) * (2 * sigma_pred_target + C2)) / \
               ((mu_pred_sq + mu_target_sq + C1) * (sigma_pred_sq + sigma_target_sq + C2))

    return ssim_map.mean().item()


def calculate_rmse(pred, target):
    """è®¡ç®—RMSE"""
    mse = F.mse_loss(pred, target)
    rmse = torch.sqrt(mse)
    return rmse.item()


# ========== æ•°æ®é›†ç±»ï¼ˆé€‚é…HSI-MSIï¼‰==========
class HSI_MSI_TestDataset(data.Dataset):
    def __init__(self, hsi_dir, msi_dir, gt_dir, target_size=128):
        """
        Args:
            hsi_dir: ä½åˆ†è¾¨ç‡HSIç›®å½• (Z_reconst/)
            msi_dir: é«˜åˆ†è¾¨ç‡MSIç›®å½• (Y_reconst/)
            gt_dir: é«˜åˆ†è¾¨ç‡GTç›®å½• (X/)
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸ï¼ˆé»˜è®¤128ï¼‰
        """
        super(HSI_MSI_TestDataset, self).__init__()

        self.target_size = target_size

        self.hsi_paths = self.find_mat_files(hsi_dir)
        self.msi_paths = self.find_mat_files(msi_dir)
        self.gt_paths = self.find_mat_files(gt_dir)

        assert len(self.hsi_paths) == len(self.msi_paths) == len(self.gt_paths), \
            f"æ•°æ®æ•°é‡ä¸ä¸€è‡´: HSI={len(self.hsi_paths)}, MSI={len(self.msi_paths)}, GT={len(self.gt_paths)}"

        print(f"âœ… åŠ è½½äº† {len(self.hsi_paths)} å¯¹æµ‹è¯•æ ·æœ¬ (ç›®æ ‡å°ºå¯¸: {target_size}Ã—{target_size})")

    def find_mat_files(self, dir_path):
        """æŸ¥æ‰¾æ‰€æœ‰.matæ–‡ä»¶"""
        mat_files = []
        for root, dirs, files in os.walk(dir_path):
            for file in files:
                if file.endswith('.mat'):
                    mat_files.append(os.path.join(root, file))
        mat_files.sort()
        return mat_files

    def read_mat_image(self, path):
        """è¯»å–.matæ–‡ä»¶ï¼ˆä¸train.pyä¸€è‡´ï¼‰"""
        try:
            mat_data = sio.loadmat(path)
            valid_keys = [k for k in mat_data.keys() if not k.startswith('__')]
            if len(valid_keys) == 0:
                raise ValueError(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®é”®: {path}")
            key = valid_keys[0]
            img = mat_data[key]

            # æ™ºèƒ½è¯†åˆ«é€šé“ç»´åº¦
            if img.ndim == 2:
                img = torch.from_numpy(img).float().unsqueeze(0)
            elif img.ndim == 3:
                shape = img.shape
                expected_channels = [3, 31]
                channel_dim_idx = None

                for i, s in enumerate(shape):
                    if s in expected_channels:
                        channel_dim_idx = i
                        break

                if channel_dim_idx is not None:
                    target_dim = channel_dim_idx
                else:
                    target_dim = np.argmin(shape)

                img = np.moveaxis(img, source=target_dim, destination=0)
                img = torch.from_numpy(img).float()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç»´åº¦: {img.ndim}Dï¼Œè·¯å¾„: {path}")

            # å½’ä¸€åŒ–
            if img.max() > 1.0:
                img = img / img.max()

            return img

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {path}")
            print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            raise

    def __getitem__(self, index):
        hsi = self.read_mat_image(self.hsi_paths[index])  # (31, H_low, W_low)
        msi = self.read_mat_image(self.msi_paths[index])  # (3, H_high, W_high)
        gt = self.read_mat_image(self.gt_paths[index])  # (31, H_high, W_high)

        # âœ… Resizeåˆ°ç›®æ ‡å°ºå¯¸
        hsi_target_size = self.target_size // 32  # ä¿æŒ32å€ä¸‹é‡‡æ ·æ¯”ä¾‹

        hsi = F.interpolate(
            hsi.unsqueeze(0), size=(hsi_target_size, hsi_target_size),
            mode='bilinear', align_corners=False
        ).squeeze(0)

        msi = F.interpolate(
            msi.unsqueeze(0), size=(self.target_size, self.target_size),
            mode='bilinear', align_corners=False
        ).squeeze(0)

        gt = F.interpolate(
            gt.unsqueeze(0), size=(self.target_size, self.target_size),
            mode='bilinear', align_corners=False
        ).squeeze(0)

        file_name = os.path.basename(self.hsi_paths[index])

        return hsi, msi, gt, file_name

    def __len__(self):
        return len(self.hsi_paths)


# ========== è·¯å¾„é…ç½® ==========
hsi_test_dir = r"D:/datas/CAVEdata/Z_reconst"  # âœ… ä¿®æ”¹ä¸ºä½ çš„æµ‹è¯•HSIè·¯å¾„
msi_test_dir = r"D:/datas/CAVEdata/Y_reconst"  # âœ… ä¿®æ”¹ä¸ºä½ çš„æµ‹è¯•MSIè·¯å¾„
gt_test_dir = r"D:/datas/CAVEdata/X"  # âœ… ä¿®æ”¹ä¸ºä½ çš„æµ‹è¯•GTè·¯å¾„

save_dir = "./test_results"
save_fusion_dir = os.path.join(save_dir, "fusion")
save_metrics_dir = os.path.join(save_dir, "metrics")

utils.check_dir(save_dir)
utils.check_dir(save_fusion_dir)
utils.check_dir(save_metrics_dir)

# ========== æ•°æ®åŠ è½½å™¨ ==========
test_dataset = HSI_MSI_TestDataset(
    hsi_test_dir,
    msi_test_dir,
    gt_test_dir,
    target_size=args.args.img_size  # ä½¿ç”¨argsä¸­çš„å°ºå¯¸ï¼ˆé»˜è®¤128ï¼‰
)

test_data_iter = data.DataLoader(
    dataset=test_dataset,
    shuffle=False,
    batch_size=1,
    num_workers=0
)

# ========== åŠ è½½æ¨¡å‹ ==========
print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
with torch.no_grad():
    base_msi = model.base(in_channels=3)  # MSI: 3é€šé“
    base_hsi = model.base(in_channels=31)  # HSI: 31é€šé“
    hsi_MFE = model.FeatureExtractor()
    msi_MFE = model.FeatureExtractor()
    fusion_decoder = model.Decoder()
    PAFE = model.FeatureExtractor()
    decoder = model.Decoder()
    MN_hsi = model.Enhance()
    MN_msi = model.Enhance()
    HSIDP = model.DictionaryRepresentationModule()
    MSIDP = model.DictionaryRepresentationModule()
    MHCSA_hsi = model.MHCSAB()
    MHCSA_msi = model.MHCSAB()
    fusion_module = model.FusionMoudle()

# ========== åŠ è½½é¢„è®­ç»ƒæƒé‡ ==========
pretrain_dir = r"./checkpoints/train_models/20250120_12-00-00_MulFS-CAP-HSI-MSI_model"  # âœ… ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
checkpoint_name = "epoch99_iter100.pth"  # âœ… é€‰æ‹©æœ€ä½³æ¨¡å‹

checkpoint_path = os.path.join(pretrain_dir, checkpoint_name)

if not os.path.exists(checkpoint_path):
    print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    print(f"è¯·æ£€æŸ¥è·¯å¾„æˆ–ä¿®æ”¹ pretrain_dir å’Œ checkpoint_name")
    exit(1)

print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {checkpoint_path}")
checkpoints = torch.load(checkpoint_path, map_location=device)

# âœ… åŠ è½½æ‰€æœ‰æ¨¡å—çš„æƒé‡
utils.load_state_dir(base_msi, checkpoints['bfe_msi'], device)
utils.load_state_dir(base_hsi, checkpoints['bfe_hsi'], device)
utils.load_state_dir(msi_MFE, checkpoints['msi_mfe'], device)
utils.load_state_dir(hsi_MFE, checkpoints['hsi_mfe'], device)
utils.load_state_dir(PAFE, checkpoints['pafe'], device)
utils.load_state_dir(fusion_decoder, checkpoints['fusion_decoder'], device)
utils.load_state_dir(decoder, checkpoints['decoder'], device)
utils.load_state_dir(MSIDP, checkpoints['msi_dgfp'], device)
utils.load_state_dir(HSIDP, checkpoints['hsi_dgfp'], device)
utils.load_state_dir(MN_msi, checkpoints['mn_msi'], device)
utils.load_state_dir(MN_hsi, checkpoints['mn_hsi'], device)
utils.load_state_dir(MHCSA_msi, checkpoints['mhcsab_msi'], device)
utils.load_state_dir(MHCSA_hsi, checkpoints['mhcsab_hsi'], device)
utils.load_state_dir(fusion_module, checkpoints['fusion_block'], device)

# âœ… è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
base_msi.eval()
base_hsi.eval()
msi_MFE.eval()
hsi_MFE.eval()
PAFE.eval()
fusion_decoder.eval()
decoder.eval()
MSIDP.eval()
HSIDP.eval()
MN_msi.eval()
MN_hsi.eval()
MHCSA_msi.eval()
MHCSA_hsi.eval()
fusion_module.eval()

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# ========== æµ‹è¯•å¾ªç¯ ==========
print("ğŸš€ å¼€å§‹æµ‹è¯•...")

all_metrics = {
    'PSNR': [],
    'SAM': [],
    'SSIM': [],
    'RMSE': []
}

results_per_image = []  # å­˜å‚¨æ¯å¼ å›¾åƒçš„è¯¦ç»†ç»“æœ

for x in tqdm(test_data_iter, desc="æµ‹è¯•è¿›åº¦"):
    hsi, msi, gt, file_name = x
    file_name = file_name[0]  # ä»tupleä¸­æå–å­—ç¬¦ä¸²

    hsi = hsi.to(device)  # (1, 31, 4, 4) - å¦‚æœtarget_size=128
    msi = msi.to(device)  # (1, 3, 128, 128)
    gt = gt.to(device)  # (1, 31, 128, 128)

    with torch.no_grad():
        # ========== ä¸Šé‡‡æ ·HSIåˆ°MSIåˆ†è¾¨ç‡ ==========
        hsi_up = F.interpolate(
            hsi,
            size=(msi.size(2), msi.size(3)),
            mode='bilinear',
            align_corners=False
        )

        # ========== ç‰¹å¾æå– ==========
        hsi_base = base_hsi(hsi_up)  # (1, 64, 128, 128)
        msi_base = base_msi(msi)  # (1, 64, 128, 128)

        hsi_fe = hsi_MFE(hsi_base)
        msi_fe = msi_MFE(msi_base)

        hsi_f = PAFE(hsi_base)
        msi_f = PAFE(msi_base)

        # ========== æ¨¡æ€å½’ä¸€åŒ– + å­—å…¸è¡¥å¿ ==========
        hsi_e_f = MN_hsi(hsi_f)
        msi_e_f = MN_msi(msi_f)

        HSIDP_hsi_f, _ = HSIDP(hsi_e_f)
        MSIDP_msi_f, _ = MSIDP(msi_e_f)

        # ========== è·¨æ¨¡æ€å¯¹é½æ„ŸçŸ¥ ==========
        fixed_DP = HSIDP_hsi_f
        moving_DP = MSIDP_msi_f

        moving_DP_lw = model.df_window_partition(
            moving_DP, args.args.large_w_size, args.args.small_w_size
        )
        fixed_DP_sw = model.window_partition(
            fixed_DP, args.args.small_w_size, args.args.small_w_size
        )

        correspondence_matrixs = model.CMAP(
            fixed_DP_sw, moving_DP_lw, MHCSA_hsi, MHCSA_msi, True
        )

        # ========== ç‰¹å¾é‡ç»„ + èåˆ ==========
        msi_f_sample = model.feature_reorganization(correspondence_matrixs, msi_fe)
        fusion_image = fusion_module(hsi_fe, msi_f_sample)  # (1, 31, 128, 128)

        # ========== è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ==========
        psnr = calculate_psnr(fusion_image, gt, data_range=1.0)
        sam = calculate_sam(fusion_image, gt)
        ssim = calculate_ssim(fusion_image, gt, data_range=1.0)
        rmse = calculate_rmse(fusion_image, gt)

        all_metrics['PSNR'].append(psnr)
        all_metrics['SAM'].append(sam)
        all_metrics['SSIM'].append(ssim)
        all_metrics['RMSE'].append(rmse)

        results_per_image.append({
            'filename': file_name,
            'PSNR': psnr,
            'SAM': sam,
            'SSIM': ssim,
            'RMSE': rmse
        })

        # ========== ä¿å­˜èåˆç»“æœ ==========
        output_path = os.path.join(save_fusion_dir, file_name)
        sio.savemat(output_path, {'data': fusion_image.squeeze(0).cpu().numpy()})

        # ========== æ¸…ç†æ˜¾å­˜ ==========
        del hsi_up, hsi_base, msi_base, hsi_fe, msi_fe
        del hsi_f, msi_f, hsi_e_f, msi_e_f
        del HSIDP_hsi_f, MSIDP_msi_f, fixed_DP, moving_DP
        del correspondence_matrixs, msi_f_sample, fusion_image
        torch.cuda.empty_cache()

# ========== ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV ==========
import csv

csv_path = os.path.join(save_metrics_dir, 'results_per_image.csv')
with open(csv_path, 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['filename', 'PSNR', 'SAM', 'SSIM', 'RMSE'])
    writer.writeheader()
    writer.writerows(results_per_image)

print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {csv_path}")

# ========== æ‰“å°ç»Ÿè®¡ç»“æœ ==========
print("\n" + "=" * 70)
print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
print("=" * 70)

for metric_name, values in all_metrics.items():
    mean_val = np.mean(values)
    std_val = np.std(values)
    min_val = np.min(values)
    max_val = np.max(values)
    print(f"{metric_name:8s}: {mean_val:8.4f} Â± {std_val:6.4f}  (min: {min_val:7.4f}, max: {max_val:7.4f})")

print("=" * 70)
print(f"âœ… èåˆç»“æœå·²ä¿å­˜è‡³: {save_fusion_dir}")
print(f"âœ… è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³: {csv_path}")
print("=" * 70)