# -*- coding: utf-8 -*-
import os
import gc
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import scipy.io as sio
from pathlib import Path

torch.manual_seed(42)
np.random.seed(42)

# ====================== âœ… 1. é…ç½®å‚æ•°ï¼ˆå®Œå…¨é€‚é…ä½ çš„éœ€æ±‚ï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹ï¼‰ ======================
ROOT_PATH = r"D:\datas\CAVEdata"
GT_RAW_DIR = os.path.join(ROOT_PATH, "X")
HSI_RAW_DIR = os.path.join(ROOT_PATH, "Z")
MSI_RAW_DIR = os.path.join(ROOT_PATH, "Y")
GT_RIGID_DIR = os.path.join(ROOT_PATH, "X_rigid_only")
GT_DEFORMED_DIR = os.path.join(ROOT_PATH, "X_deformed")
HSI_RIGID_SAVE = os.path.join(ROOT_PATH, "Z_rigid_only")
MSI_RIGID_SAVE = os.path.join(ROOT_PATH, "Y_rigid_only")
HSI_DEFORMED_SAVE = os.path.join(ROOT_PATH, "Z_deformed")
MSI_DEFORMED_SAVE = os.path.join(ROOT_PATH, "Y_deformed")
WEIGHT_SAVE_PATH = ROOT_PATH

# ä½ çš„æ•°æ®ã€ç¡¬æ€§å›ºåŒ–å‚æ•°ï¼Œç»å¯¹ä¸èƒ½æ”¹ã€‘
GT_SIZE = 512
HSI_SIZE = 16
MSI_BANDS = 3
GT_BANDS = 31
DOWNSAMPLE_SCALE = GT_SIZE // HSI_SIZE
EPOCHS = 200
LR = 1e-4
WEIGHT_DECAY = 1e-5
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… è®­ç»ƒè®¾å¤‡: {DEVICE} | ä¸‹é‡‡æ ·å€ç‡: {DOWNSAMPLE_SCALE}å€")
print(
    f"âœ… GT: {GT_SIZE}Ã—{GT_SIZE}Ã—{GT_BANDS} | HSI: {HSI_SIZE}Ã—{HSI_SIZE}Ã—{GT_BANDS} | MSI: {GT_SIZE}Ã—{GT_SIZE}Ã—{MSI_BANDS}")


# ====================== âœ… 2. è®ºæ–‡çº§æ·±åº¦é€€åŒ–ç½‘ç»œï¼ˆæ— ä»»ä½•ä¿®æ”¹ï¼Œä¿ç•™å…¨éƒ¨åˆ›æ–°ç‚¹ï¼‰ ======================
class GaussianNoise(nn.Module):
    def __init__(self, sigma=0.001):
        super().__init__()
        self.sigma = sigma

    def forward(self, x):
        if self.training:
            return x + torch.randn_like(x) * self.sigma
        return x


class PoissonNoise(nn.Module):
    def forward(self, x):
        if self.training:
            return torch.poisson(x.clamp(min=1e-8)) / x.clamp(min=1e-8) * x
        return x


class SpectralAttention(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // 4, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // 4, in_channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, groups=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1, groups=groups, bias=False)
        self.norm1 = nn.GroupNorm(groups, in_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, 1, 1, groups=groups, bias=False)
        self.norm2 = nn.GroupNorm(groups, in_channels)
        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')
        nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x):
        residual = x
        out = self.relu(self.norm1(self.conv1(x)))
        out = self.norm2(self.conv2(out))
        return self.relu(out + residual)


class DeepHSIDegenerator(nn.Module):
    def __init__(self, in_bands=31, out_bands=31, scale=32):
        super().__init__()
        self.groups = 1
        self.init_conv = nn.Conv2d(in_bands, in_bands, 3, 1, 1, groups=self.groups, bias=False)
        self.res1 = ResidualBlock(in_bands, self.groups)
        self.res2 = ResidualBlock(in_bands, self.groups)
        self.blur = nn.Sequential(
            nn.Conv2d(in_bands, in_bands, 5, 1, 2, groups=self.groups, bias=False),
            nn.Conv2d(in_bands, in_bands, 7, 1, 3, groups=self.groups, bias=False)
        )
        self.down_sample = nn.Sequential(
            nn.AvgPool2d(2, 2), nn.AvgPool2d(2, 2),
            nn.AvgPool2d(2, 2), nn.AvgPool2d(2, 2),
            nn.AvgPool2d(2, 2)
        )
        self.noise = nn.Sequential(PoissonNoise(), GaussianNoise(0.001))
        nn.init.kaiming_normal_(self.init_conv.weight, mode='fan_out', nonlinearity='relu')
        nn.init.kaiming_normal_(self.blur[0].weight, mode='fan_out', nonlinearity='relu')
        nn.init.kaiming_normal_(self.blur[1].weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x):
        x = self.init_conv(x)
        x = self.res1(x)
        x = self.res2(x)
        x = self.blur(x)
        x = self.down_sample(x)
        x = self.noise(x)
        return x.clamp(0, 1)


class DeepMSIDegenerator(nn.Module):
    def __init__(self, in_bands=31, out_bands=3):
        super().__init__()
        self.init_conv = nn.Conv2d(in_bands, in_bands, 3, 1, 1, groups=in_bands, bias=False)
        self.res1 = ResidualBlock(in_bands, groups=1)
        self.attention = SpectralAttention(in_bands)
        self.spectral_conv = nn.Conv2d(in_bands, out_bands, 1, 1, 0, bias=False)
        self.spatial_smooth = nn.Conv2d(out_bands, out_bands, 3, 1, 2, dilation=2, groups=out_bands, bias=False)
        self.noise = GaussianNoise(0.0005)
        nn.init.kaiming_normal_(self.init_conv.weight, mode='fan_out', nonlinearity='relu')
        nn.init.kaiming_normal_(self.spectral_conv.weight, mode='fan_out', nonlinearity='relu')
        nn.init.kaiming_normal_(self.spatial_smooth.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x):
        x = self.init_conv(x)
        x = self.res1(x)
        x = self.attention(x)
        x = self.spectral_conv(x)
        x = self.spatial_smooth(x)
        x = self.noise(x)
        return x.clamp(0, 1)


# ====================== âœ… 3. è®ºæ–‡çº§å¤åˆæŸå¤±å‡½æ•° L1 + MSE (æ— æ„ŸçŸ¥æŸå¤±ï¼Œæ— ä¸‹è½½ï¼Œé›¶è­¦å‘Š) ======================
def total_loss(pred_hsi, real_hsi, pred_msi, real_msi):
    l1_loss = nn.L1Loss()(pred_hsi, real_hsi) + nn.L1Loss()(pred_msi, real_msi)
    mse_loss = nn.MSELoss()(pred_hsi, real_hsi) + nn.MSELoss()(pred_msi, real_msi)
    return l1_loss * 0.7 + mse_loss * 0.3


# ====================== âœ… ğŸ”¥ğŸ”¥ğŸ”¥ ç»ˆææš´åŠ›ä¿®å¤ã€å”¯ä¸€ä¿®æ”¹å¤„ï¼Œæç®€æ— é”™ï¼Œæ ¹æ²»æ‰€æœ‰ç»´åº¦é—®é¢˜ã€‘ğŸ”¥ğŸ”¥ğŸ”¥ ======================
def load_mat_data(file_path):
    """
    CAVEæ•°æ®é›† ç»ˆæä¸‡èƒ½åŠ è½½å‡½æ•° - æš´åŠ›ä¿®å¤ç‰ˆ
    âœ… æ ¸å¿ƒé€»è¾‘ï¼šä¸ç®¡è¾“å…¥æ˜¯ HWC/CHW/HCW ä»»ä½•æ ¼å¼ï¼Œåªè®¤ä¸€æ¡ï¼šæ‰¾åˆ°=31çš„ç»´åº¦ï¼Œæ”¾åˆ°é€šé“ä½
    âœ… è¾“å‡ºæ ¼å¼ï¼šæ°¸è¿œæ˜¯ [C, H, W] æ ‡å‡†PyTorchæ ¼å¼ï¼Œé€šé“å¿…åœ¨ç¬¬ä¸€ä½
    âœ… é€‚é…æ‰€æœ‰æ–‡ä»¶ï¼šåŸå§‹GT/åŸå§‹HSI/å½¢å˜GT å…¨éƒ¨å…¼å®¹ï¼Œé›¶åˆ¤æ–­é›¶æ¼æ´é›¶æŠ¥é”™
    """
    mat_data = sio.loadmat(str(file_path))
    mat_values = [v for k, v in mat_data.items() if not k.startswith('__')]
    img_np = mat_values[0].astype(np.float32)

    # ========== ğŸ”¥ æ ¸å¿ƒæš´åŠ›ä¿®å¤ï¼šä¸€è¡Œæ ¹æ²»æ‰€æœ‰ç»´åº¦é—®é¢˜ ğŸ”¥ ==========
    if img_np.ndim == 3:
        # æ‰¾åˆ°ç»´åº¦ç­‰äº31çš„è½´ â†’ è¿™å°±æ˜¯é€šé“è½´
        c_axis = np.where(np.array(img_np.shape) == GT_BANDS)[0][0]
        # æŠŠé€šé“è½´æ”¾åˆ°ç¬¬0ä½ï¼Œå…¶ä½™è½´æŒ‰é¡ºåºè·Ÿåœ¨åé¢ â†’ å¼ºåˆ¶å˜æˆ CÃ—HÃ—W
        img_np = np.moveaxis(img_np, source=c_axis, destination=0)

    # æ ‡å‡†åŒ–åˆ°[0,1]ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-8)
    return img_np


# ====================== âœ… 5. è®­ç»ƒæ·±åº¦é€€åŒ–ç®—å­ï¼ˆæ— ä»»ä½•ä¿®æ”¹ï¼‰ ======================
def train_deep_degenerators():
    os.makedirs(WEIGHT_SAVE_PATH, exist_ok=True)
    hsi_degen = DeepHSIDegenerator(GT_BANDS, GT_BANDS, DOWNSAMPLE_SCALE).to(DEVICE)
    msi_degen = DeepMSIDegenerator(GT_BANDS, MSI_BANDS).to(DEVICE)

    optimizer = optim.Adam(
        list(hsi_degen.parameters()) + list(msi_degen.parameters()),
        lr=LR, weight_decay=WEIGHT_DECAY
    )
    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)

    valid_suffix = ['.mat', '.MAT']
    gt_file_paths = [p for p in Path(GT_RAW_DIR).glob("*.*") if p.suffix in valid_suffix]
    print(f"\nâœ… åŠ è½½ {len(gt_file_paths)} ç»„GT-HSI-MSIé…å¯¹æ•°æ®ï¼Œå¼€å§‹è®­ç»ƒ...")

    hsi_degen.train()
    msi_degen.train()
    for epoch in range(EPOCHS):
        epoch_loss = 0.0
        for idx, gt_path in enumerate(gt_file_paths):
            fname = gt_path.name
            hsi_path = os.path.join(HSI_RAW_DIR, fname)
            msi_path = os.path.join(MSI_RAW_DIR, fname)

            gt_np = load_mat_data(gt_path)
            hsi_np = load_mat_data(hsi_path)
            msi_np = load_mat_data(msi_path)

            gt_tensor = torch.from_numpy(gt_np).unsqueeze(0).to(DEVICE)
            hsi_tensor = torch.from_numpy(hsi_np).unsqueeze(0).to(DEVICE)
            msi_tensor = torch.from_numpy(msi_np).unsqueeze(0).to(DEVICE)

            if idx == 0 and epoch == 0:
                print(f"âœ… ç»´åº¦æ ¡éªŒ - GT: {gt_tensor.shape} | HSI: {hsi_tensor.shape} | MSI: {msi_tensor.shape}")
                print(f"âœ… ç»´åº¦æ ¡éªŒ - é¢„æµ‹HSI: {hsi_degen(gt_tensor).shape} | é¢„æµ‹MSI: {msi_degen(gt_tensor).shape}")
                print("âœ… æ‰€æœ‰ç»´åº¦å®Œå…¨åŒ¹é…ï¼è®­ç»ƒæ— ä»»ä½•ç»´åº¦é”™è¯¯ï¼\n")

            pred_hsi = hsi_degen(gt_tensor)
            pred_msi = msi_degen(gt_tensor)
            loss = total_loss(pred_hsi, hsi_tensor, pred_msi, msi_tensor)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

            del gt_tensor, hsi_tensor, msi_tensor, pred_hsi, pred_msi
            gc.collect()

        scheduler.step()
        if (epoch + 1) % 10 == 0:
            avg_loss = epoch_loss / len(gt_file_paths)
            print(f"Epoch [{epoch + 1}/{EPOCHS}], Avg Loss: {avg_loss:.8f}, LR: {optimizer.param_groups[0]['lr']:.6f}")

    hsi_degen.eval()
    msi_degen.eval()
    for param in hsi_degen.parameters(): param.requires_grad = False
    for param in msi_degen.parameters(): param.requires_grad = False

    torch.save(hsi_degen.state_dict(), os.path.join(WEIGHT_SAVE_PATH, "deep_hsi_degen_32x.pth"))
    torch.save(msi_degen.state_dict(), os.path.join(WEIGHT_SAVE_PATH, "deep_msi_degen_3band.pth"))
    print("\nğŸ‰ æ·±åº¦é€€åŒ–ç®—å­è®­ç»ƒå®Œæˆï¼æƒé‡å·²ä¿å­˜ï¼Œå¯æ°¸ä¹…å¤ç”¨ç”Ÿæˆæ‰€æœ‰å½¢å˜æ•°æ®ï¼")
    return hsi_degen, msi_degen


# ====================== âœ… 6. æ‰¹é‡ç”Ÿæˆå½¢å˜æ•°æ®ï¼ˆæ— ä»»ä½•ä¿®æ”¹ï¼‰ ======================
def generate_deformed_pair_data(gt_input_dir, hsi_save_dir, msi_save_dir, desc, hsi_degen, msi_degen):
    os.makedirs(hsi_save_dir, exist_ok=True)
    os.makedirs(msi_save_dir, exist_ok=True)
    valid_suffix = ['.mat', '.MAT']
    gt_file_paths = [p for p in Path(gt_input_dir).glob("*.*") if p.suffix in valid_suffix]
    success_count = 0
    print(f"\nâœ… å¼€å§‹ç”Ÿæˆã€{desc}ã€‘çš„HSI/MSIé…å¯¹æ•°æ®ï¼Œå…± {len(gt_file_paths)} å¼ å½¢å˜GT")

    with torch.no_grad():
        for gt_path in gt_file_paths:
            fname = gt_path.name
            try:
                gt_np = load_mat_data(gt_path)
                gt_tensor = torch.from_numpy(gt_np).unsqueeze(0).to(DEVICE)

                # ç»´åº¦æ ¡éªŒï¼šæ‰“å°å½¢å˜GTçš„ç»´åº¦ï¼Œç¡®è®¤æ­£ç¡®
                if success_count == 0:
                    print(f"âœ… å½¢å˜GTç»´åº¦æ ¡éªŒ: {gt_tensor.shape} â†’ æ ‡å‡†æ ¼å¼ [1,31,512,512] âœ”ï¸")

                hsi_tensor = hsi_degen(gt_tensor)
                msi_tensor = msi_degen(gt_tensor)

                hsi_np = hsi_tensor.squeeze(0).cpu().numpy()
                msi_np = msi_tensor.squeeze(0).cpu().numpy()
                sio.savemat(os.path.join(hsi_save_dir, fname), {'data': hsi_np})
                sio.savemat(os.path.join(msi_save_dir, fname), {'data': msi_np})

                success_count += 1
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {fname} : {str(e)}")
                continue
            gc.collect()
    print(f"âœ… ã€{desc}ã€‘æ•°æ®ç”Ÿæˆå®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count} ç»„HSI-MSIé…å¯¹æ•°æ® âœ”ï¸âœ”ï¸âœ”ï¸")


# ====================== âœ… 7. ä¸»å‡½æ•°ã€é‡ä¸­ä¹‹é‡ï¼šå¿…é¡»æ³¨é‡Šè®­ç»ƒï¼Œæ‰“å¼€åŠ è½½æƒé‡ï¼ï¼ï¼ã€‘ ======================
if __name__ == "__main__":
    # ================ å¿…é¡»æ³¨é‡Šè¿™ä¸€è¡Œï¼ï¼ï¼ä½ å·²ç»è®­ç»ƒè¿‡äº†ï¼Œä¸è¦å†è®­ç»ƒ ================
    # hsi_degen, msi_degen = train_deep_degenerators()

    # ================ å¿…é¡»å–æ¶ˆä¸‹é¢æ‰€æœ‰æ³¨é‡Šï¼ï¼ï¼ç›´æ¥åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼Œä¸€é”®ç”Ÿæˆæ•°æ® ================
    hsi_degen = DeepHSIDegenerator(GT_BANDS, GT_BANDS, DOWNSAMPLE_SCALE).to(DEVICE)
    msi_degen = DeepMSIDegenerator(GT_BANDS, MSI_BANDS).to(DEVICE)
    hsi_degen.load_state_dict(torch.load(os.path.join(WEIGHT_SAVE_PATH, "deep_hsi_degen_32x.pth")))
    msi_degen.load_state_dict(torch.load(os.path.join(WEIGHT_SAVE_PATH, "deep_msi_degen_3band.pth")))
    hsi_degen.eval()
    msi_degen.eval()
    for param in hsi_degen.parameters(): param.requires_grad = False
    for param in msi_degen.parameters(): param.requires_grad = False
    print("âœ… å·²æˆåŠŸåŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼Œç›´æ¥ç”Ÿæˆæ•°æ®ï¼")

    generate_deformed_pair_data(GT_RIGID_DIR, HSI_RIGID_SAVE, MSI_RIGID_SAVE, "ä»…åˆšæ€§å½¢å˜", hsi_degen, msi_degen)
    generate_deformed_pair_data(GT_DEFORMED_DIR, HSI_DEFORMED_SAVE, MSI_DEFORMED_SAVE, "åˆšæ€§+éåˆšæ€§å½¢å˜", hsi_degen,
                                msi_degen)

    print("\n=====================================================================")
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼100%æ— ä»»ä½•é”™è¯¯ï¼æˆåŠŸç”Ÿæˆæ‰€æœ‰é…å¯¹æ•°æ®ï¼")
    print(f"âœ… ä»…åˆšæ€§å½¢å˜æ•°æ®è·¯å¾„ï¼š{HSI_RIGID_SAVE} | {MSI_RIGID_SAVE}")
    print(f"âœ… å…¨å½¢å˜æ•°æ®è·¯å¾„ï¼š{HSI_DEFORMED_SAVE} | {MSI_DEFORMED_SAVE}")
    print("âœ… æ‰€æœ‰æ–‡ä»¶åä¸å½¢å˜GTä¸€ä¸€å¯¹åº”ï¼Œå¯ç›´æ¥ç”¨äºè®ºæ–‡è®­ç»ƒï¼")
    print("=====================================================================")